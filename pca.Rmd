---
title: "PCA"
author: "M Whalen"
date: "2024-02-27"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Example to Start
1. Generate and view sample.
```{r}
# Step 1: Generate sample data
set.seed(123)
x1 <- rnorm(100)  # Sample data for dimension 1
x2 <- 2*x1 + rnorm(100)  # Sample data for dimension 2

data <- tibble(x1 = x1, x2 = x2)

ggplot(data, aes(x1,x2)) +
  geom_point()
```

2. Covariance structure of data

```{r}
# Step 2: Compute the covariance matrix
covariance_matrix <- cov(data)
covariance_matrix
```

3. Transform data to communicate variability. Use Eigenvalues/vectors

```{r}
# Step 3: Compute the eigenvalues and eigenvectors
eigen_info <- eigen(covariance_matrix)
eigenvalues <- eigen_info$values
eigenvectors <- eigen_info$vectors
eigenvalues
eigenvectors
```

4. Order in greatest variation first. "get PCA"

```{r}
# Step 4: Sort eigenvalues in decreasing order
sorted_indices <- order(eigenvalues, decreasing = TRUE)
sorted_eigenvalues <- eigenvalues[sorted_indices]
sorted_eigenvectors <- eigenvectors[, sorted_indices]
```

5. Choose principal components, first 1 since most of the variabaility. Transform the data.

```{r}
# Step 5: Choose the principal components
# For demonstration purposes, let's keep only the first principal component
pc1 <- sorted_eigenvectors[, 1]

# Proportion of the first PC
sorted_eigenvalues[1]/sum(sorted_eigenvalues)

# Project the original data onto the principal component
projected_data <- as.matrix(data) %*% pc1

# Reconstruct the data from the projected data
reconstructed_data <- projected_data %*% t(pc1)


# Convert data to a data frame
original_df <- data.frame(x = data[,1], y = data[,2])
reconstructed_df <- data.frame(x = reconstructed_data[,1], y = reconstructed_data[,2])

#original_df
#reconstructed_df
```

6. Plot and "see" reduction of dimension.

```{r}
# Plot original and reconstructed data using ggplot
ggplot() +
  geom_point(data = original_df, aes(x1, x2), color = "black", size = 2) +
  geom_point(data = reconstructed_df, aes(x, y), color = "red", size = 2) +
  labs(x = "Dimension 1", y = "Dimension 2", title = "Original and Reconstructed Data") +
  theme_minimal()

proj <- data.frame(first_pca = projected_data, y = 0)
ggplot(proj, aes(first_pca, y)) +
  geom_point()
```

## Equivariance and Scaling
Equivariant method is where results of the method arrange appropriately when data is transformed.

### Ex Regression equation aka Transformation

$\hat{Y}_{i}=2+4 X_{j}$

let $z_{j}=x_{j} / 2$ then for $z_{j}$ new estimated regression is

$\widehat{y}_{j}^{15}=2+8z_i$ since $8z_i = 8\frac{x_{i}}{2} = 4x_i$

for $z=1$ the model is 10, which is same as $x=2$ which is also 10.

Least squares regression is equivariant with respect to location and scale.
**Important PCA is _not_ equivariant with respect to scale.**

Eg. If $S = \left(\begin{array}{ll} 2 &1 \\1 & 2 \end{array}\right)$ for $x_{j1}, x_{j2}$ $j = 1, \dots, n$

eigen values and eigen vectors $\hat{\lambda}_{1}=3 \quad \hat{\lambda}_{2}=1$, $\hat{e}_{1}=\left(\begin{array}{l}1 / \sqrt{2} \\ 1 / \sqrt{2}\end{array}\right) \hat{e}_{2}=\left(\begin{array}{l}-1 / \sqrt{2} \\ 1 / \sqrt{2} \end{array}\right).$

So first $PC$ is

$$\frac{1}{\sqrt{2}} x_{j,1}+\frac{1}{\sqrt{2}} x_{j,2}=.707 x_{j 1}+.707 x_{j 2}$$
with proportion of variability explained: $\frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}}=\frac{3}{4} =75 \%$

Now we rescale the analysis,

$y_{j 1}=\frac{x_{j 1}}{2} \quad \& \quad y_{j 2}=x_{j 2}$ so using var/cor properties

$S_{y}=\left[\begin{array}{cc}\frac{2}{4} & \frac{1}{2} \\ \frac{1}{2} & 2\end{array}\right]$ how eigenvalues are $\lambda_{1} = 2.15 \quad \& \quad \lambda_{2}=0.35$ with $\hat{e}_{1}=\left(\begin{array}{l}.29 \\ .957\end{array}\right)$ $\hat{e}_{2}=\left(\begin{array}{r}-.957 \\ .290\end{array}\right)$ so first PC $= .290 y_{i 1}+.957 y_{j 2}$ but when it is written as $x$'s $.29\left(\frac{x_{j 1}}{2}\right)+.957 x_{j 2}=.145 x_{j 1}+.957 y_{j 2}$.


If we rescale so coefficient vector is length 1 $.150 x_{j 1}+.989 x_{j 2}$ not what we got before...

What about proportion of variance from 1st PC? It is $\frac{2.15}{2.15 + .35}=.86$ not 75% like previously.

So scaling choice is meaningful! If $x$'s are large magnitudes different, then that difference matters for PCA result.

Eg. $x_{1}=$ height $x_{2}=$ weight.

1. $x_{1}$ in meters and $x_{2}$ in grams, where $x_{2}$ has more variance and large coeff.

2. $x_{1}$ in millimeters $\frac{1}{}$ weight in tons, now $x_1$ has more variance and 1st PC will mainly be about is $x_{1}$

*most* of the time variables are scaled by standard deviation unless on same scale like the iris dataset,  when we do that its talking about correlation. But this is a conscious choice! We are saying that each variable considered the same ~ish.

### Covariance and Correlation

- use PCA on covariance matrix if all variables are on the same scale (roughly)... but hard in real life 
- otherwise should be using $R$ correlation matrix

Calculating eigenvalues eigenvectors in $R$ is equivalent to calculating the PC of the original after they'd been standardized to have unit variance. When we do this it is a deliberate choice stating that all variables should be "equally" weighted.

When performing Principal Component Analysis (PCA), it's common to work with either the covariance matrix or the correlation matrix of the original variables. While both matrices provide insights into the underlying structure of the data, they can yield slightly different principal components due to scaling differences.

- Covariance Matrix PCA (Nonscaled):

  - Principal components represent directions of maximum variance in the original data space.
  - Variables with larger variances have a greater influence on the principal components.
  - Loadings reflect the covariance between original variables and principal components.
  - Scaling: Variables are not standardized, so their variances directly affect the principal components.

- Correlation Matrix PCA (Scaled):

  - Principal components represent directions of maximum correlation between variables.
  - Variables are standardized, making them comparable regardless of scale.
  - Loadings reflect the correlation between original variables and principal components.
  - Scaling: Variables are standardized to have unit variance, ensuring equal contribution to principal components.

- Relating Principal Components:

  - Projection: Project principal components from one analysis onto the space spanned by principal components from the other analysis.
  - Correlation: Calculate correlation between principal components obtained from covariance and correlation matrices.
  - Rotation: Use rotation techniques to align principal components from both analyses.
  - Eigenvalue Comparison: Compare eigenvalues obtained from both PCA analyses to assess variance explained.

**Interpretation:** While the direction and magnitude of principal components may differ between covariance and correlation matrices, they capture similar underlying structures.
Using correlation matrices is often preferred when variables have different scales, as it ensures equal contribution to principal components.

## Choosing Number of PC
We noted that the optimal choice for approx $x_{j}$ is $v_{1} v_{2} \ldots v_{r}$ where they are $\hat{e}_{1} \hat{e}_{2} \ldots \hat{e}_{r}$ eigenvectors from the $r$ largest eigenvalues. But how large should it be? How many eigenvectors are needed so that for $j=1 \ldots n$ of $x_{j}$ can be accurately represented by its coordinates wrt $\hat{e}_{1} \hat{e}_{2} \ldots \hat{e}_{r}$?

Recall that, $\frac{\hat{\lambda}_{1}+\hat{\lambda}_{2}+\ldots+\hat{\lambda}_{r}}{\hat{\lambda}_{1}+\hat{\lambda}_{2}+\ldots+\hat{\lambda}_{p}}$ maybe interpreted as a measure of how well observed data is approximated by linear function of first $r$ eigenvectors. The "proportion of variance explained".

Since these values are in decreasing order of $\hat{\lambda}_{i}$ is a small value that suggest $\hat{e}_{i} \hat{e}_{i+1} \ldots \hat{e}_{p}$ can be ignored without losing information.

Although definitive answers exist people look at the following

1. The proportion of total variance attributed to the first $j$ principal components. As $j$ increases this ratio generally increases until $j>p$ and it approaches 1. So we pick an $r$ to be the smallest $j$ for which the ratio exceds some threshold, generally. $.7-.9$.

2. The average total variance per variable,  $\frac{1}{p} \sum_{j=1}^{p} \hat{\lambda}_{j}=\frac{1}{p} \operatorname{tr}(S)$ , PC for which $\hat{\lambda}_{i}<\frac{1}{p} \sum^p{\hat{\lambda_j}}$ that is for which
$\frac{\lambda_{i}}{\sum \lambda_{j}}<\frac{1}{p}$  are ignored.
Note that if the analysis is based on correlation $\sum_{j=1}^{p} \hat{\lambda}_{j}=p$ so that PC for which $\hat{\lambda}_{i}<1$ are ignored.

3. A plot of the estimated eigenvalues $\hat{\lambda}_{i}$ for $i=1 \ldots . p$ versus $i$ called a scree plot. Typically the plot has $\hat{\lambda}_{i}$ increasing rapidly as $i$ decreases from 1 and there is a value of $j$ at which $\hat{\lambda}_{i}$ for $i=j+1, j+2, \ldots . p$ are roughly equal (platues) Then ignore those $j+1, j+2, \ldots P$ PCS.

## Interpretations

To interpret PCA effectively, we focus on understanding the relationship between original variables and principal components, primarily through correlation loadings and component coefficients.

### Principal Component Scores (observations):
Principal component scores represent the projection of the original data onto the new orthogonal axes defined by the principal components. These scores can be used for data visualization, clustering, or further analysis. Positive scores indicate that the observation is in the direction of increasing values for the corresponding principal component, while negative scores indicate the opposite.

**Interpretation:** When interpreting principal component scores, consider the direction and magnitude of the scores. Observations with high positive scores on a particular principal component are characterized by high values of the variables that contribute positively to that component. Conversely, observations with high negative scores exhibit low values of those variables.


### Loadings/Rotations:
Definition: Loadings are coefficients that quantify the contribution of each original variable to each principal component. They indicate the strength and direction of the relationship between the variables and the principal components.

**Interpretation:** Loadings are calculated based on the cosine of the angle of rotation between the principal component axes and the original axes. A positive loading indicates that a variable positively contributes to the principal component, meaning higher values of the variable are associated with higher scores on the component. Conversely, a negative loading indicates that the absence of the variable contributes positively to the component.

- Magnitude: The magnitude of a loading reflects the importance of the variable to the principal component. Larger loadings indicate stronger relationships between variables and components, suggesting that the variable has a greater impact on the overall structure of the data captured by that component.

- Range: Loadings range from -1 to +1, where -1 indicates a perfect negative correlation between the variable and the component, +1 indicates a perfect positive correlation, and 0 indicates no correlation. The sign of the loading indicates the direction of the relationship (positive or negative), while the magnitude indicates the strength.

-  Importance: Variables with high loadings on a particular principal component are considered important for explaining the variation captured by that component. These variables contribute significantly to the underlying structure of the data represented by the component.


## Biplots
### Representation of Variables:
In a biplot, the original variables are represented as vectors in the same space as the principal components. The direction and length of each vector indicate the relationship between the original variables and the principal components. Variables that point in similar directions are positively correlated, while those pointing in opposite directions are negatively correlated.

### Contribution of Variables:
The length of each vector in a biplot represents the importance or contribution of the corresponding variable to the principal components. Longer vectors indicate variables with greater influence on the principal components and thus on the overall structure of the data.

### Interpretation of Points/Scores:
Observations (data points) from the original dataset are also plotted in the biplot. The position of each point relative to the vectors of the original variables and the principal components provides insights into the relationships between observations and variables.

### Proximity of Points and Vectors:
Points that are close to the vectors of the original variables indicate high values for those variables, while points far from the vectors indicate low values. Similarly, the proximity of points to each other reflects similarities or differences between observations based on their values across the original variables.

### Clustering and Grouping:
Biplots can reveal patterns of clustering or grouping among observations and variables. Observations that cluster together in the biplot share similar characteristics based on their values across the original variables. Similarly, variables that cluster together in the biplot are strongly correlated with each other and may represent underlying patterns or dimensions in the data.

### Interpretation of Principal Components:
In addition to interpreting the relationships between variables and observations, biplots allow for the interpretation of principal components. The direction and length of the principal component vectors provide insights into the relative importance of each component and the contribution of original variables to each component.

### Overall Structure:
By examining the arrangement of variables, observations, and principal components in the biplot, analysts can gain a comprehensive understanding of the overall structure of the data. Biplots facilitate the identification of patterns, trends, and relationships that may not be apparent from the original dataset alone.

## Using PCA for variable selection in regression 
Consider a regression model with response variable $y$ and predictor $x_{1} x_{2} \ldots x_{p}$


$y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{p} x_{p}+\varepsilon$ where $E\left(y \mid x_{1} x_{2} \ldots x_{p}\right)=0$. 

Suppose $x_{1} x_{2} \ldots x_{p}$ is a good predictor of $y$ but they are correlated with each other, accurate estimation of $\beta_{i}$ 's is difficult due to standard errors of the estimate being large. Also correlation suggest not all predictors are necessary. What can you do?


Reduce set of predictor with PCA: 

Linear functions of $x_{1} \ldots x_{p}$ that are useful for predicting $y$ have a large variation. So constant predictors - unlikely to be helpful.
Thus, use PCA to identify linear functions of the predictors that have large variance. This would be using the first few linear PC as predictors. ~ this may he helpful but don't use $y$ only $x_{i}$'s also no math reason for using exact $P C$ in the regression either ~

Formal PCR steps
1. Use PCA on $x_{1} \ldots x_{P}$
2. Choose first few PC and compute PC scores $z_{1} z_{2} \ldots z_{q}$
3. Fit model with predictors $z_{1} z_{2} \ldots z_{q}$ using least squares.
4. Use coefficients from this model and equation for each $z_{j}$ to obtain estimates of coeff. for each $x_{k}$.


## Seasame Street Data
Load in data and select variables of interest (pre test). What's the correlation structure.

```{r}
# Load the Sesame Street dataset
sesame <- read_csv("data/sesame.csv")

# Select only pre-test variables
pre_sesame <- sesame %>% 
  select(starts_with("pre")) %>% 
  select(-pretest)

# Compute correlation matrix
correlation_matrix <- cor(pre_sesame)
correlation_matrix
```

### Run PCA

prcomp outputs

sdev - the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).

rotation - the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors). 

Note: princomp was written for compatiblity with S-PLUS however it is not recommended. Its is better to use prcomp or svd. That is because by default princomp performs a decompostion of the covariance not correlation matrix. princomp can call eigen on the correlation or covariance matrix. Its default calculation uses divisor N for the covariance matrix.

```{r}
# ?prcomp

# Run PCA with scaling
pre_pca <- prcomp(pre_sesame, scale = TRUE)
pre_pca
summary(pre_pca)

```

### Manually using PCA and Subsetting

```{r}
# Perform PCA manually with selected variables
manual_pca <- prcomp(~prebody+prelet+preform+prenumb+prerelat+preclasf, scale = TRUE, data = sesame)

# Perform PCA with subset (e.g., by gender)
gender_pca <- prcomp(~prebody+prelet+preform+prenumb+prerelat+preclasf, subset = (sex==2),
                     scale = TRUE, data = sesame)
```

### Tidyverse way
https://clauswilke.com/blog/2020/09/07/pca-tidyverse-style/
```{r}
library(broom)
# Perform PCA using tidyverse
tidy_pca <- sesame %>%
  select(sex, prebody, prelet, preform, prenumb, prerelat, preclasf) %>% 
  group_by(sex) %>%
  nest() %>% 
  mutate(pca = map(data, ~ prcomp(.x, scale = TRUE)))
tidy_pca$pca 

```

### Scaling Data
```{r}
# Perform PCA without scaling
noscale_pca <- prcomp(pre_sesame, scale = FALSE)
noscale_pca
map(pre_sesame, sd)

# Scale data manually
scaled_presesame <- scale(pre_sesame, center = FALSE, scale = c(32, 58, 20, 54, 17, 24))
scaled_prcomp <- prcomp(scaled_presesame, scale = FALSE)

```


### Calculating PC Scores

```{r}
# Calculate PC scores
pc_scores <- predict(pre_pca)
head(pc_scores)

```

### How many PC ?
Scree Plot
```{r}
# Scree plot
screeplot(pre_pca)
```

## Interpretation 
Look at the scaled by equation question.
```{r}
# View PCA summary
print(scaled_prcomp)
summary(scaled_prcomp)

# Extract loadings/rotations
loadings_matrix <- scaled_prcomp$rotation
## First PC loadings
loadings_matrix[,1]
## First 2 PC
loadings_matrix[,1:2]

# Extract component coefficients (eigenvectors)
coefficients_matrix <- loadings_matrix * sqrt(scaled_prcomp$sdev)
coefficients_matrix

#
eig <- eigen(cov(scaled_presesame))
ord_id <- order(eig$values, decreasing = TRUE)
sorted_eigenvalues <- eig$values[ord_id]
sorted_eigenvectors <- eig$vectors[, ord_id]

sorted_eigenvalues
scaled_prcomp$sdev^2

sorted_eigenvectors
loadings_matrix
```


## Factoretra

```{r}
library(factoextra)
library(broom)
fviz_screeplot(pre_pca)
fviz_pca(pre_pca) ## too much sometimes
## look at variables
fviz_pca_var(pre_pca,
             col.var = "contrib", # Color by contributions to the PC
                                 gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                                 repel = TRUE) # Avoid text

## look at indiviudals but still a lot to view
fviz_pca_ind(pre_pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
### pulling out rotations 
sesame %>%
  select(sex, prebody, prelet, preform, prenumb, prerelat, preclasf) %>% 
  group_by(sex) %>%
  nest() %>% 
  mutate(pca = map(data, ~ prcomp(.x, scale = TRUE)),
         pca_rot = map(pca, ~ .x %>% 
                         tidy(matrix = "rotation"))) %>% ## Get rotations for each variable for each PC
  unnest(pca_rot)

## can pull out rotation, loadings, eigenvalues in tidy format
```

### Try it with Post Test seasame

## Other Datasets to Practice PCA

- USAArrests
- Palmerpenguins
- mtcars
- diamonds
- decathalon
- iris...
